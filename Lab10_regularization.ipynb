{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 10: Regularization to Prevent Overfitting: L1, L2 and Dropout\n",
    "\n",
    "#### CSC 215 Artificial Intelligence (Spring 2019)\n",
    "\n",
    "#### Dr. Haiquan Chen, California State University, Sacramento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Functions for Tensorflow (Little Gems)\n",
    "\n",
    "The following functions will be used with TensorFlow to help preprocess the data.  They allow you to build the feature vector for a neural network. \n",
    "\n",
    "* Predictors/Inputs \n",
    "    * Fill any missing inputs with the median for that column.  Use **missing_median**.\n",
    "    * Encode textual/categorical values with **encode_text_dummy**.\n",
    "    * Encode numeric values with **encode_numeric_zscore**.\n",
    "* Output\n",
    "    * Discard rows with missing outputs.\n",
    "    * Encode textual/categorical values with **encode_text_index**.\n",
    "    * Do not encode output numeric values.\n",
    "* Produce final feature vectors (x) and expected output (y) with **to_xy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df, name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name, x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df, name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "\n",
    "# Encode a numeric column as zscores\n",
    "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name] - mean) / sd\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the median\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "\n",
    "# Convert all missing values in the specified column to the default\n",
    "def missing_default(df, name, default_value):\n",
    "    df[name] = df[name].fillna(default_value)\n",
    "\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column. \n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if isinstance(target_type, collections.Sequence) else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred,y,sort=True):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Remove all rows where the specified column is +/- sd standard deviations\n",
    "def remove_outliers(df, name, sd):\n",
    "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
    "    df.drop(drop_rows, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# Encode a column to a range between normalized_low and normalized_high.\n",
    "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1,\n",
    "                         data_low=None, data_high=None):\n",
    "    if data_low is None:\n",
    "        data_low = min(df[name])\n",
    "        data_high = max(df[name])\n",
    "\n",
    "    df[name] = ((df[name] - data_low) / (data_high - data_low)) \\\n",
    "               * (normalized_high - normalized_low) + normalized_low\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 regularization techniques used in linear regression\n",
    "\n",
    "L1 and L2 regularization are two common regularization techniques.\n",
    "\n",
    "We are going to look at linear regression to see how L1 and L2 regularization work.  The following code sets up the auto-mpg data for this purpose.\n",
    "\n",
    "https://www.kaggle.com/uciml/autompg-dataset/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>429.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>4341</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford galaxie 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>454.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>4354</td>\n",
       "      <td>9.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet impala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4312</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth fury iii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>455.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>4425</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>pontiac catalina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>390.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3850</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc ambassador dpl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>383.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>3563</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>dodge challenger se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>340.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>3609</td>\n",
       "      <td>8.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth 'cuda 340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>400.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3761</td>\n",
       "      <td>9.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet monte carlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>455.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>3086</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick estate wagon (sw)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24.0</td>\n",
       "      <td>4</td>\n",
       "      <td>113.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2372</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>toyota corona mark ii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22.0</td>\n",
       "      <td>6</td>\n",
       "      <td>198.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2833</td>\n",
       "      <td>15.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth duster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6</td>\n",
       "      <td>199.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2774</td>\n",
       "      <td>15.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc hornet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>200.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2587</td>\n",
       "      <td>16.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford maverick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2130</td>\n",
       "      <td>14.5</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>datsun pl510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1835</td>\n",
       "      <td>20.5</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>volkswagen 1131 deluxe sedan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0   18.0          8         307.0       130.0    3504          12.0    70   \n",
       "1   15.0          8         350.0       165.0    3693          11.5    70   \n",
       "2   18.0          8         318.0       150.0    3436          11.0    70   \n",
       "3   16.0          8         304.0       150.0    3433          12.0    70   \n",
       "4   17.0          8         302.0       140.0    3449          10.5    70   \n",
       "5   15.0          8         429.0       198.0    4341          10.0    70   \n",
       "6   14.0          8         454.0       220.0    4354           9.0    70   \n",
       "7   14.0          8         440.0       215.0    4312           8.5    70   \n",
       "8   14.0          8         455.0       225.0    4425          10.0    70   \n",
       "9   15.0          8         390.0       190.0    3850           8.5    70   \n",
       "10  15.0          8         383.0       170.0    3563          10.0    70   \n",
       "11  14.0          8         340.0       160.0    3609           8.0    70   \n",
       "12  15.0          8         400.0       150.0    3761           9.5    70   \n",
       "13  14.0          8         455.0       225.0    3086          10.0    70   \n",
       "14  24.0          4         113.0        95.0    2372          15.0    70   \n",
       "15  22.0          6         198.0        95.0    2833          15.5    70   \n",
       "16  18.0          6         199.0        97.0    2774          15.5    70   \n",
       "17  21.0          6         200.0        85.0    2587          16.0    70   \n",
       "18  27.0          4          97.0        88.0    2130          14.5    70   \n",
       "19  26.0          4          97.0        46.0    1835          20.5    70   \n",
       "\n",
       "    origin                          name  \n",
       "0        1     chevrolet chevelle malibu  \n",
       "1        1             buick skylark 320  \n",
       "2        1            plymouth satellite  \n",
       "3        1                 amc rebel sst  \n",
       "4        1                   ford torino  \n",
       "5        1              ford galaxie 500  \n",
       "6        1              chevrolet impala  \n",
       "7        1             plymouth fury iii  \n",
       "8        1              pontiac catalina  \n",
       "9        1            amc ambassador dpl  \n",
       "10       1           dodge challenger se  \n",
       "11       1            plymouth 'cuda 340  \n",
       "12       1         chevrolet monte carlo  \n",
       "13       1       buick estate wagon (sw)  \n",
       "14       3         toyota corona mark ii  \n",
       "15       1               plymouth duster  \n",
       "16       1                    amc hornet  \n",
       "17       1                 ford maverick  \n",
       "18       3                  datsun pl510  \n",
       "19       2  volkswagen 1131 deluxe sedan  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin-1</th>\n",
       "      <th>origin-2</th>\n",
       "      <th>origin-3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>429.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>4341</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>454.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>4354</td>\n",
       "      <td>9.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4312</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>455.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>4425</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>390.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3850</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>383.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>3563</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>340.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>3609</td>\n",
       "      <td>8.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>400.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3761</td>\n",
       "      <td>9.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>455.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>3086</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24.0</td>\n",
       "      <td>4</td>\n",
       "      <td>113.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2372</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22.0</td>\n",
       "      <td>6</td>\n",
       "      <td>198.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2833</td>\n",
       "      <td>15.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6</td>\n",
       "      <td>199.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2774</td>\n",
       "      <td>15.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>200.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2587</td>\n",
       "      <td>16.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2130</td>\n",
       "      <td>14.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1835</td>\n",
       "      <td>20.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0   18.0          8         307.0       130.0    3504          12.0    70   \n",
       "1   15.0          8         350.0       165.0    3693          11.5    70   \n",
       "2   18.0          8         318.0       150.0    3436          11.0    70   \n",
       "3   16.0          8         304.0       150.0    3433          12.0    70   \n",
       "4   17.0          8         302.0       140.0    3449          10.5    70   \n",
       "5   15.0          8         429.0       198.0    4341          10.0    70   \n",
       "6   14.0          8         454.0       220.0    4354           9.0    70   \n",
       "7   14.0          8         440.0       215.0    4312           8.5    70   \n",
       "8   14.0          8         455.0       225.0    4425          10.0    70   \n",
       "9   15.0          8         390.0       190.0    3850           8.5    70   \n",
       "10  15.0          8         383.0       170.0    3563          10.0    70   \n",
       "11  14.0          8         340.0       160.0    3609           8.0    70   \n",
       "12  15.0          8         400.0       150.0    3761           9.5    70   \n",
       "13  14.0          8         455.0       225.0    3086          10.0    70   \n",
       "14  24.0          4         113.0        95.0    2372          15.0    70   \n",
       "15  22.0          6         198.0        95.0    2833          15.5    70   \n",
       "16  18.0          6         199.0        97.0    2774          15.5    70   \n",
       "17  21.0          6         200.0        85.0    2587          16.0    70   \n",
       "18  27.0          4          97.0        88.0    2130          14.5    70   \n",
       "19  26.0          4          97.0        46.0    1835          20.5    70   \n",
       "\n",
       "    origin-1  origin-2  origin-3  \n",
       "0          1         0         0  \n",
       "1          1         0         0  \n",
       "2          1         0         0  \n",
       "3          1         0         0  \n",
       "4          1         0         0  \n",
       "5          1         0         0  \n",
       "6          1         0         0  \n",
       "7          1         0         0  \n",
       "8          1         0         0  \n",
       "9          1         0         0  \n",
       "10         1         0         0  \n",
       "11         1         0         0  \n",
       "12         1         0         0  \n",
       "13         1         0         0  \n",
       "14         0         0         1  \n",
       "15         1         0         0  \n",
       "16         1         0         0  \n",
       "17         1         0         0  \n",
       "18         0         0         1  \n",
       "19         0         1         0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create feature vector\n",
    "missing_median(df, 'horsepower')\n",
    "df.drop('name', 1, inplace=True)\n",
    "\n",
    "encode_text_dummy(df, 'origin')\n",
    "df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode to a 2D matrix for training\n",
    "x,y = to_xy(df,'mpg')\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=45) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Linear Regression\n",
    "\n",
    "To understand L1/L2 regularization, it is good to start with linear regression.  L1/L2 were first introduced for [linear regression](https://en.wikipedia.org/wiki/Linear_regression).  They can also be used for neural networks.  \n",
    "\n",
    "The following code uses linear regression to fit the auto-mpg data set.  The RMSE reported will not be as good as a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 2.937157154083252\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create linear regression\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# Fit/train linear regression\n",
    "regressor.fit(x_train,y_train)\n",
    "\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "\n",
    "print(\"Final score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin-1', 'origin-2', 'origin-3']\n"
     ]
    }
   ],
   "source": [
    "names = list(df.columns.values)\n",
    "\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.416543  ,  0.02445623, -0.00778467, -0.00747326,  0.13812245,\n",
       "        0.8012743 , -1.4483743 ,  0.8185649 ,  0.6298092 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-18.257574"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to evaluate the coefficients of a regression\n",
    "\n",
    "%matplotlib inline    \n",
    "from IPython.display import display   \n",
    "\n",
    "def report_coef(names,coef,intercept):\n",
    "    r = pd.DataFrame( { 'coef': coef, 'positive': coef>=0  }, index = names )\n",
    "    r = r.sort_values(by=['coef'])\n",
    "    display(r)\n",
    "    print(\"Intercept: {}\".format(intercept))\n",
    "    r['coef'].plot(kind='barh', color=r['positive'].map({True: 'b', False: 'r'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>origin-1</th>\n",
       "      <td>-1.448374</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cylinders</th>\n",
       "      <td>-0.416543</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horsepower</th>\n",
       "      <td>-0.007785</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>-0.007473</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displacement</th>\n",
       "      <td>0.024456</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acceleration</th>\n",
       "      <td>0.138122</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin-3</th>\n",
       "      <td>0.629809</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.801274</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin-2</th>\n",
       "      <td>0.818565</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coef  positive\n",
       "origin-1     -1.448374     False\n",
       "cylinders    -0.416543     False\n",
       "horsepower   -0.007785     False\n",
       "weight       -0.007473     False\n",
       "displacement  0.024456      True\n",
       "acceleration  0.138122      True\n",
       "origin-3      0.629809      True\n",
       "year          0.801274      True\n",
       "origin-2      0.818565      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -18.2575740814209\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names.remove(\"mpg\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 (Lasso) Regularization\n",
    "\n",
    "L1 Regularization, also called LASSO (Least Absolute Shrinkage and Selection Operator) should be used to create sparsity in the neural network. \n",
    "\n",
    "### L1 algorithm will push many weight connections to near 0.  \n",
    "\n",
    "When a weight is near 0, the program drops it from the network.  Dropping weighted connections will create a sparse neural network.\n",
    "\n",
    "### The lower weight values will typically lead to less overfitting.\n",
    "\n",
    "### If your data set has a large number of input features that may not be needed, L1 regularization can help to detect and ignore unnecessary features.\n",
    "\n",
    "\n",
    "### Minimization objective = SSE (Sum of Squared Error) + $\\alpha$ * (Sum of Absolute Value of Coefficients)\n",
    "\n",
    "When $\\alpha$ is 0, Lasso regression produces the same coefficients as a linear regression. When $\\alpha$ is very very large, all coefficients are zero.\n",
    "\n",
    "The following code demonstrates lasso regression.  Notice the effect of the coefficients compared to the previous section that used linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 3.040905714035034\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>origin-1</th>\n",
       "      <td>-1.264474</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>-0.007458</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horsepower</th>\n",
       "      <td>-0.002797</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cylinders</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin-2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin-3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displacement</th>\n",
       "      <td>0.013005</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acceleration</th>\n",
       "      <td>0.113760</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.787195</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coef  positive\n",
       "origin-1     -1.264474     False\n",
       "weight       -0.007458     False\n",
       "horsepower   -0.002797     False\n",
       "cylinders    -0.000000      True\n",
       "origin-2      0.000000      True\n",
       "origin-3      0.000000      True\n",
       "displacement  0.013005      True\n",
       "acceleration  0.113760      True\n",
       "year          0.787195      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -17.271265029907227\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create linear regression\n",
    "regressor = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit/train LASSO\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df.columns.values)\n",
    "names.remove(\"mpg\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 (Ridge) Regularization\n",
    "\n",
    "Use Ridge/L2 regularization when you prefer low weight values.  ***The lower weight values will typically lead to less overfitting.*** \n",
    "\n",
    "\n",
    "### Minimization objective = SSE (Sum of Squared Error) + $\\alpha$ * (Sum of Square of Coefficients)\n",
    "\n",
    "When $\\alpha$ is 0, L2 regression produces the same coefficients as a linear regression. When $\\alpha$ is very very large, all coefficients are zero.\n",
    "\n",
    "### L1 will force the weights into a pattern similar to a laplace distribution; the L2 will force the weights into a pattern similar to a Gaussian distribution***, as demonstrated the following:\n",
    "\n",
    "![L1 vs L2](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_9_l1_l2.png \"L1 vs L2\")\n",
    "\n",
    "\n",
    "The following code uses L2 with linear regression (Ridge regression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 2.9375734329223633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:112: LinAlgWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number3.884085e-10\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>origin-1</th>\n",
       "      <td>-1.445044</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cylinders</th>\n",
       "      <td>-0.415839</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horsepower</th>\n",
       "      <td>-0.007759</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>-0.007473</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displacement</th>\n",
       "      <td>0.024422</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acceleration</th>\n",
       "      <td>0.138103</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin-3</th>\n",
       "      <td>0.629260</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.801221</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin-2</th>\n",
       "      <td>0.817394</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coef  positive\n",
       "origin-1     -1.445044     False\n",
       "cylinders    -0.415839     False\n",
       "horsepower   -0.007759     False\n",
       "weight       -0.007473     False\n",
       "displacement  0.024422      True\n",
       "acceleration  0.138103      True\n",
       "origin-3      0.629260      True\n",
       "year          0.801221      True\n",
       "origin-2      0.817394      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -18.25517463684082\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create linear regression\n",
    "regressor = Ridge(alpha=0.1)\n",
    "\n",
    "# Fit/train Ridge\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df.columns.values)\n",
    "names.remove(\"mpg\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet:   Linear regression with mixing L1 with L2.  \n",
    "    \n",
    "* $\\alpha$: Constant for penalty (regularization). \n",
    "    \n",
    "* l1_ratio : The ElasticNet mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty.\n",
    "\n",
    "\n",
    "#### If you want the following:  a $*$ L1 + b $*$ L2,   set $\\alpha$ = a + b and l1_ratio = a / (a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 3.031985282897949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>origin-1</th>\n",
       "      <td>-0.938924</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cylinders</th>\n",
       "      <td>-0.257568</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>-0.007462</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horsepower</th>\n",
       "      <td>-0.002896</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displacement</th>\n",
       "      <td>0.017533</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acceleration</th>\n",
       "      <td>0.131320</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin-3</th>\n",
       "      <td>0.369088</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin-2</th>\n",
       "      <td>0.458725</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.788913</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coef  positive\n",
       "origin-1     -0.938924     False\n",
       "cylinders    -0.257568     False\n",
       "weight       -0.007462     False\n",
       "horsepower   -0.002896     False\n",
       "displacement  0.017533      True\n",
       "acceleration  0.131320      True\n",
       "origin-3      0.369088      True\n",
       "origin-2      0.458725      True\n",
       "year          0.788913      True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -17.4800968170166\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAD8CAYAAADJ7YuWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHXpJREFUeJzt3XuUXGWd7vHvQ0AQghAkOohgAJEMiiakRFBuaga8EwQEvBzuOXhUVE6Yo0thBMXrrGEcZyHGqIEZBxFIBBk0hEsIlwToxNy4I5AZFbURRIISMDznj3qbKZq+VF+qa3fzfNbq1bve/e53/2pT9JN3165dsk1ERESVbdTuAiIiIvqTsIqIiMpLWEVEROUlrCIiovISVhERUXkJq4iIqLyEVUREVF7CKiIiKi9hFRERlbdxuwsYK7bddltPmjSp3WVERIway5Yte9j2xGb6JqyGyaRJk+jo6Gh3GRERo4aktc32zWnAiIiovIRVRERUXsIqIiIqL2EVERGVlwssIuJ5pHZXEKPFSH0lYmZWERFReQmriIiovIRVRERUXsKqSZLGtbuGiIgXqjEZVpK+KOmTDY/PlnSKpNMk3SZplaQzG9b/RNIySbdLmtnQvk7SWZJuAfYZ4acRERHFmAwr4HvAMQCSNgKOAn4H7ArsBUwBpknav/Q/3vY0oAacIumlpX0LYI3tN9m+sftOJM2U1CGpo7Ozs7XPKCLiBWxMhpXtB4E/SJoKHAT8Anhjw/JyYDL18IJ6QK0ElgI7NLRvAC7tYz+zbdds1yZObOpejBERMQhj+XNWc4Bjgb8Bvg+8HfiK7e80dpJ0IDAd2Mf2nyUtAjYrq5+0vWGkCo6IiJ6NyZlVMR94B/UZ1YLyc7yk8QCStpf0MmAr4NESVJOBvdtVcERE9GzMzqxsPyXpOuCPZXZ0laS/BZao/vH8dcCHgZ8DJ0taBdxN/VRgRERUyJgNq3Jhxd7AEV1ttr8JfLOH7u/saQzb41tTXUREDMSYDCtJuwNXAPNt39vueiJGm5G631tEs8ZkWNm+A9i53XVERMTwGMsXWERExBiRsIqIiMpLWEVEROUlrCIiovISVhERUXkJq4iIqLyEVUREVF7CKiIiKi9hFRERlZewioiIyktYRURE5Y3JewNGxNDUv0UnhiI3Ax5emVlFRETljdqwknSlpK376XOWpOkDHPeHku6WtEbS9yVtMrRKIyJiqEZdWKluI9vvsv3HvvraPsP21QPcxQ+BycAewIuBEwdZakREDJNKhpWkU8vMZo2kT0maJOlOSecCy4EdJD0oadvS/3RJd0laKOlCSbNK+1xJh5flByWdKWm5pNWSJve0b9tXugBuBV45Ms86IiJ6U7mwkjQNOA54E/WvpT8JmADsBlxge6rttQ39a8BhwFTg/UCtj+Eftr0n8G1gVj91bAJ8BPj54J9NREQMh8qFFbAv9a+jf8L2OmAesB+w1vbSXvpfZvsvth8HftrH2PPK72XApH7qOBdYbPuG3jpImimpQ1JHZ2dnP8NFRMRgVTGserto9okB9u/J+vJ7A+WyfUkLJK2QNOfZAaV/ACYCp/Y1mO3Ztmu2axMnThxAGRERMRBVDKvFwAxJm0vaAjgU6HV2A9wIvFfSZpLGA+8eyM5sH2x7iu0TASSdCBwMHG37mcE9hYiIGE6V+1Cw7eWS5lK/uAFgDvBoH/1vk3Q5sBJYC3QAjw2hhPPKOEtU/2TkPNtnDWG8iIgYInkMfMxa0njb6yRtTn1mNtP28pGsoVaruaOjYyR3GdEyuYPF0I2BP60tJ2mZ7b4uintW5WZWgzRb0u7AZsD5Ix1UERHRWmMirGx/sN01RIwlmRVE1VTxAouIiIjnSFhFRETlJawiIqLyElYREVF5CauIiKi8hFVERFRewioiIiovYRUREZWXsIqIiMpLWEVEROUlrCIiovLGxL0BI2J45a7ruT9i1WRmFRERlTeqw0rSlZK27qfPWZKmD3Dc70laKWmVpEvKNxBHRESbjMqwUt1Gtt9l+4999bV9hu2rB7iLT9t+g+3XA/8FfHzQxUZExJBVNqwknSppTfn5lKRJku6UdC6wHNhB0oOSti39T5d0l6SFki6UNKu0z5V0eFl+UNKZkpZLWi1pck/7tv2n0l/Ai4GcvY6IaKNKhpWkacBxwJuAvYGTgAnAbsAFtqfaXtvQvwYcBkwF3g/09TXJD9veE/g2MKuPGn4A/BaYDHxrSE8oIiKGpJJhBewLzLf9hO11wDxgP2Ct7aW99L/M9l9sPw78tI+x55Xfy4BJvXWyfRzwCuBO4Mie+kiaKalDUkdnZ2d/zykiIgapqmHV24WzTwywf0/Wl98bKJfuS1ogaYWkOY0dbW8ALqI+a3se27Nt12zXJk6cOIASIiJiIKoaVouBGZI2l7QFcChwQx/9bwTeK2mzcuXeuweyM9sH255i+8Ry8car4dn3rN4L3DW4pxEREcOhkh8Ktr1c0lzg1tI0B3i0j/63SbocWAmsBTqAxwa5ewHnS3pJWV4JfHSQY0VExDCQx8jHtCWNt71O0ubUZ2YzbS8fqf3XajV3dHSM1O4iWip3sMgdLEaCpGW2+7og7lmVnFkN0mxJuwObAeePZFBFRERrjZmwsv3BdtcQMVZkVhFVU9ULLCIiIp6VsIqIiMpLWEVEROUlrCIiovISVhERUXkJq4iIqLyEVUREVF7CKiIiKi9hFRERlZewioiIyktYRURE5Y2ZewNGxPBp5V3Xc9/BGIzMrCIiovIqGVaSFklq6jtOmhhrRvnqkK7HZ0maPhxjR0TEyKhkWA2UpHF9rJ4BPBtWts+wfXXrq4qIiOEypLCS9BNJyyTdLmlmaXuHpOWSVkq6prSNl/QDSaslrZJ0WGk/SNKS0v9iSeN72EePfSQ9KOkMSTcCR0g6SdJtZb+XStpc0puB9wHfkLRC0i6S5ko6vIzxdkm/KHV9X9KmDWOfWfa5WtLkoRyniIgYmqHOrI63PQ2oAadIejnwXeAw228Ajij9Tgces72H7dcD10raFvg8MN32nkAHcGrj4E30edL2vrZ/BMyz/cay3zuBE2zfDFwOnGZ7iu1fNoy9GTAXONL2HtQvNvlow9gPl31+G5g1xOMUERFDMNSrAU+RdGhZ3gGYCSy2/QCA7UfKuunAUV0b2X5U0nuon567SfVLj14ELOk2/t799LmoYfl1kr4EbA2MBxb0U/tuwAO27ymPzwc+BvxzeTyv/F4GvL+nAcpscibAjjvu2M/uIiJisAYdVpIOpB5C+9j+s6RFwErqIfC87kD3C1YFLLR9dF+76afPEw3Lc4EZtldKOhY4sL+n0M/69eX3Bno5TrZnA7MBarVaLsiNiGiRoZwG3Ap4tATVZOqzoE2BAyTtBCBpm9L3KuDjXRtKmgAsBd4i6dWlbXNJr+m2j2b6dNkSeEjSJsCHGtofL+u6uwuY1DU28BHg+iaed0REjLChhNXPgY0lrQK+SD1YOqmfFpsnaSX/c5ruS8AESWtK+1ttdwLHAheWMZYCz7mQoZk+DU4HbgEWUg+iLj8CTisXUuzSMPaTwHHAxZJWA88A5w3mQERERGvJ+Tj5sKjVau7o6Gh3GRHDInewiJEgaZntpj5TOyY+ZxUREWNb7g0YEc+T2U9UTWZWERFReQmriIiovIRVRERUXsIqIiIqL2EVERGVl7CKiIjKS1hFRETlJawiIqLyElYREVF5CauIiKi8hFVERFRe7g0YEc/TeNf13CcwqiAzq4iIqLwBz6wkfQFYB7wEWGz76gFufyAwy/Z7BrrvkSZpBnCP7TvaXUtExAvZoGdWts8YaFCNQjOA3dtdRETEC11TYSXpc5LulnQ1sFtpmyvp8LL8VUl3SFol6R8b1p8n6QZJ90h63kxK0l6Sbi5fOX+zpK6xx0n6R0mry5ifKO3TJF0vaZmkBZK2K+2LJJ0jabGkOyW9UdI8SfdK+lLD/j4s6VZJKyR9R9K40r5O0tmSVkpaKunlkt4MvA/4Rum/yxCOc0REDEG/pwElTQOOAqaW/suBZQ3rtwEOBSbbtqStGzafBBwA7AJcJ+nV3Ya/C9jf9l8lTQe+DBwGzAR2AqaWddtI2gT4FnCI7U5JRwJnA8eXsZ6yvb+kTwKXAdOAR4BfSjoHeBlwJPAW209LOhf4EHABsAWw1PbnJH0dOMn2lyRdDlxh+5J+j2RERLRMM+9Z7QfMt/1ngPIHvNGfgCeBOZL+E7iiYd2PbT8D3CvpfmByt223As6XtCtgYJPSPh04z/ZfAWw/Iul1wOuAhapfqjQOeKhhrK66VgO3236o1Hs/sAOwL/UAu61s/2Lg92WbpxrqXgb8XRPHBUkzqQcrO+64YzObRETEIDR7gUWvF6+Wmc9ewNupz8A+Drytl+26P/4icJ3tQyVNAhaVdvXQV9RDaJ9eSllffj/TsNz1eOOy/fm2P9vDtk/bz16gu4Emj4vt2cBsgFqtlgt8IyJapJn3rBYDh0p6saQtgfc2rpQ0HtjK9pXAp4ApDauPkLRReb9nZ+DubmNvBfy6LB/b0H4VcLKkjcs+tinbTpS0T2nbRNJrm6i/yzXA4ZJe1jWmpFf1s83jwJYD2EdERLRAv2FlezlwEbACuBS4oVuXLYErJK0Crgc+3bDu7tL2M+Bk20922/brwFck3UT9tF6XOcB/AaskrQQ+aPsp4HDga6VtBfDmpp5l/XncAXweuKrUuhDYrp/NfgScVi4AyQUWERFtIrfo4+mS5vICujihVqu5o6Oj3WVEDIvcwSJGgqRltmvN9M0dLCIiovJadm9A28e2auyIaK3MpqJqMrOKiIjKS1hFRETlJawiIqLyElYREVF5CauIiKi8hFVERFRewioiIiovYRUREZWXsIqIiMpLWEVEROUlrCIiovISVhERUXkJq4iIqLyEVUREVN6whJWkSZLWDMdYERER3bV9ZiWpZd+pNZxGS50REWPRcIbVOEnflXS7pKskvVjSFElLJa2SNF/SBABJiyR9WdL1wCclHSFpjaSVkhaXPuMkfUPSbWX7/13aD5S0uIx3h6TzJG1U1h0taXUZ62ul7QOS/qksf1LS/WV5F0k3luVpkq6XtEzSAknb9VTnMB6riIgYgOGcLewKHG37JEk/Bg4D/h74hO3rJZ0F/APwqdJ/a9sHAEhaDRxs+9eSti7rTwAes/1GSZsCN0m6qqzbC9gdWAv8HHi/pJuBrwHTgEeBqyTNABYDp5Xt9gP+IGl7YF/gBkmbAN8CDrHdKelI4Gzg+O51didpJjATYMcddxzscYuIiH4MZ1g9YHtFWV4G7EL9D/31pe184OKG/hc1LN8EzC0hN6+0HQS8XtLh5fFW1APxKeBW210zpAupB8/TwCLbnaX9h8D+tn8iabykLYEdgP8A9qceXPOA3YDXAQslAYwDHuqlzuewPRuYDVCr1fJF4BERLTKcYbW+YXkDsHVvHYsnuhZsnyzpTcC7gRWSpgCiPitb0LiRpAOB7sHg0r83S4DjgLuBG6jPmvYB/i+wI3C77X36qzMiItqjlRdYPAY8Kmm/8vgjwPU9dZS0i+1bbJ8BPEx9BrQA+Gg5TYek10jaomyyl6SdyntVRwI3ArcAB0jaVtI44OiG/S0GZpXfvwDeCqy3/Rj1AJsoaZ+yn00kvXb4DkNERAxVq69wOwY4T9LmwP3UZzc9+YakXanPjq4BVgKrgEnActXPz3UCM0r/JcBXgT2oB9B8289I+ixwXRnnStuXlf43UA/AxbY3SPpv4C4A20+VU43/Imkr6sfkn4Hbh+kYRETEEMkeXW+1lNOAs2y/p921NKrVau7o6Gh3GRERo4akZbZrzfRt++esIiIi+jPqPuhqexGwqM1lRETECMrMKiIiKi9hFRERlZewioiIyktYRURE5SWsIiKi8hJWERFReQmriIiovIRVRERUXsIqIiIqL2EVERGVl7CKiOdSX18NF9EeCauIiKi8hFVERFTemA4rSXMk7d5Pn7nlyxe7t0+S9MHWVRcREc0a02Fl+0Tbdwxy80lAwioiogJGRVhJ+ntJp5TlcyRdW5bfLunfJR0kaYmk5ZIuljS+rF8kqVaWT5B0T2n7rqR/bdjF/pJulnR/wyzrq8B+klZI+vQIPt2IiOhmVIQVsBjYryzXgPGSNgH2BVYDnwem294T6ABObdxY0iuA04G9gb8DJncbf7sy1nuohxTAZ4AbbE+xfU5PRUmaKalDUkdnZ+cQn2JERPRmtITVMmCapC2B9cAS6qG1H/AXYHfgJkkrgGOAV3Xbfi/getuP2H4auLjb+p/YfqacMnx5s0XZnm27Zrs2ceLEQT2xiIjo36j4WnvbT0t6EDgOuBlYBbwV2AV4AFho++g+hujvgyPrB9A3IiJG2GiZWUH9VOCs8vsG4GRgBbAUeIukVwNI2lzSa7pteytwgKQJkjYGDmtif48DWw5X8RERMXijKaxuoP7e0hLbvwOepP6eUidwLHChpFXUw+s570nZ/jXwZeAW4GrgDuCxfva3CvirpJW5wCIior1ku901jAhJ422vKzOr+cD3bc8frvFrtZo7OjqGa7iI9pHgBfJ3IdpL0jLbtWb6jqaZ1VB9oVyAsYb6+1w/aXM9EdWUoIoKGhUXWAwH27PaXUNERAzOC2lmFRERo1TCKiIiKi9hFRERlZewioiIyktYRURE5SWsIiKi8hJWERFReQmriIiovIRVRERUXsIqIiIqL2EVERGV94K5N2BE22gUfp9nbmYbFZOZVUREVN6oCCtJcyUdXpbnSNp9gNuva01lERExEkbdaUDbJ7ZyfEmi/qWUz7RyPxER0by2zqwk/S9Jq8pXx8+X9ICkTcq6l0h6sOtxwzaLJNXK8jpJZ5ftl0p6eWnfSdISSbdJ+mK37U8r7asknVnaJkm6U9K5wHJghzKbWyNpdb7WPiKivdoWVpJeC3wOeJvtNwAnAIuAd5cuRwGX2n66j2G2AJaW7RcDJ5X2bwLftv1G4LcN+zwI2BXYC5gCTJO0f1m9G3CB7anAtsD2tl9new/gB0N9vhERMXjtnFm9DbjE9sMAth8B5gDHlfXH0X9IPAVcUZaXAZPK8luAC8vyvzX0P6j8/IL6DGoy9fACWGt7aVm+H9hZ0rckvQP4U087lzRTUoekjs7Ozn5KjYiIwWpnWAl4zvWxtm8CJkk6ABhne00/YzxtP3uN7Qae+x5cT9feCviK7Snl59W2v1fWPdFQx6PAG6jP9D5GPUSfx/Zs2zXbtYkTJ/ZTakREDFY7w+oa4AOSXgogaZvSfgH1WdFQTr3dRP00IsCHGtoXAMdLGl/2ub2kl3XfWNK2wEa2LwVOB/YcQi0RETFEbQsr27cDZwPXS1oJ/FNZ9UNgAv9zGm8wPgl8TNJtwFYN+7wK+A9giaTVwCXAlj1svz2wSNIKYC7w2SHUEhERQyRX7JPq5fNUh9j+SLtrGYhareaOjo52lxFVlDtYRPRI0jLbtWb6VupzVpK+BbwTeFe7a4mIiOqoVFjZ/kS7a4gYdpmlRAzZqLjdUkREvLAlrCIiovISVhERUXkJq4iIqLyEVUREVF7CKiIiKi9hFRERlZewioiIyktYRURE5SWsIiKi8hJWERFReZW6N+AL1mi8K3eMbbmfYVRMZlYREVF5ozasJF0paet++pwlafoAx/24pPskuXxjcEREtNmoOw0oSdS/NLLf77yyfcYgdnETcAWwaBDbRkREC1RyZiXpVElrys+nJE2SdKekc4HlwA6SHuya+Ug6XdJdkhZKulDSrNI+t3zzMKX/mZKWS1otaXJP+7b9C9sPjtBTjYiIJlQurCRNA44D3gTsDZwETAB2Ay6wPdX22ob+NeAwYCrwfqCvr0h+2PaewLeBWa15BhERMdwqF1bAvsB820/YXgfMA/YD1tpe2kv/y2z/xfbjwE/7GHte+b0MmDTUQiXNlNQhqaOzs3Oow0VERC+qGFa9Xcf9xAD792R9+b2B8n6dpAWSVkiaM4BxALA923bNdm3ixIkD3TwiIppUxbBaDMyQtLmkLYBDgRv66H8j8F5Jm0kaD7x7IDuzfbDtKbZPHHzJERHRSpULK9vLgbnArcAtwBzg0T763wZcDqykfpqvA3hssPuXdIqkXwGvBFYNZsYVERHDSx4Dn1SXNN72OkmbU5+ZzSyhN2JqtZo7OjoGt3HuYBFVMwb+LkT1SVpmu6+L4p416j5n1YvZknYHNgPOH+mgioiI1hoTYWX7g+2uYUjyr9iIiD5V7j2riIiI7hJWERFReQmriIiovIRVRERUXsIqIiIqb0x8zqoKJHUCaxuatgUeblM5g5F6Wyv1tlbqba1W1fsq203dqy5h1SKSOpr9sFsVpN7WSr2tlXpbqwr15jRgRERUXsIqIiIqL2HVOrPbXcAApd7WSr2tlXpbq+315j2riIiovMysIiKi8hJWQyDpCEm3S3pGUq9Xykh6h6S7Jd0n6TMN7TtJukXSvZIukvSiFte7jaSFZX8LJU3ooc9byzcnd/08KWlGWTdX0gMN66a0u97Sb0NDTZc3tFfx+E6RtKS8blZJOrJh3Ygc395ejw3rNy3H675y/CY1rPtsab9b0sGtqG8Q9Z4q6Y5yPK+R9KqGdT2+Ntpc77GSOhvqOrFh3THl9XOvpGMqUu85DbXeI+mPDetG7vjazs8gf4C/BXYDFgG1XvqMA34J7Ay8iPqXRO5e1v0YOKosnwd8tMX1fh34TFn+DPC1fvpvAzwCbF4ezwUOH8Hj21S9wLpe2it3fIHXALuW5VcADwFbj9Tx7ev12NDn/wDnleWjgIvK8u6l/6bATmWccRWo960Nr9GPdtXb12ujzfUeC/xrD9tuA9xffk8oyxPaXW+3/p8Avt+O45uZ1RDYvtP23f102wu4z/b9tp8CfgQcIknA24BLSr/zgRmtqxaAQ8p+mt3f4cDPbP+5pVX1bqD1Pquqx9f2PbbvLcu/AX4PNPWhyGHS4+uxW5/G53EJ8PZyPA8BfmR7ve0HgPvKeG2t1/Z1Da/RpdS/5btdmjm+vTkYWGj7EduPAguBd7Sozi4Drfdo4MIW19SjhFXrbQ/8d8PjX5W2lwJ/tP3Xbu2t9HLbDwGU3y/rp/9RPP+FeXY53XKOpE1bUWSDZuvdTFKHpKVdpywZBcdX0l7U/zX7y4bmVh/f3l6PPfYpx+8x6sezmW2H20D3eQLws4bHPb02WqnZeg8r/50vkbTDALcdTk3vs5xe3Qm4tqF5xI7vmPjyxVaSdDXwNz2s+pzty5oZooc299E+JH3VO8BxtgP2ABY0NH8W+C31P7Czgf8HnDW4Sp/dz3DUu6Pt30jaGbhW0mrgTz30q9rx/TfgGNvPlOZhP7497bqHtu7HZURfs/1oep+SPgzUgAMamp/32rD9y562HybN1PtT4ELb6yWdTH0W+7Ymtx1uA9nnUcAltjc0tI3Y8U1Y9cP29CEO8Stgh4bHrwR+Q/0+W1tL2rj867WrfUj6qlfS7yRtZ/uh8sfy930M9QFgvu2nG8Z+qCyul/QDYFYV6i2n07B9v6RFwFTgUip6fCW9BPhP4PO2lzaMPezHtwe9vR576vMrSRsDW1F/77KZbYdbU/uUNJ36PxgOsL2+q72X10Yrw6rfem3/oeHhd4GvNWx7YLdtFw17hc81kP+mRwEfa2wYyeOb04Ctdxuwq+pXpr2I+n/wy11/d/I66u8LARwDNDNTG4rLy36a2d/zzk2XP8Bd7wfNANa0oMZG/dYraULX6TJJ2wJvAe6o6vEtr4H5wAW2L+62biSOb4+vx259Gp/H4cC15XheDhxVrhbcCdgVuLUFNQ6oXklTge8A77P9+4b2Hl8bFah3u4aH7wPuLMsLgINK3ROAg3jumY221Ftq3o36RR9LGtpG9viO1JUcY/EHOJT6v0zWA78DFpT2VwBXNvR7F3AP9X9xfK6hfWfq/7PfB1wMbNriel8KXAPcW35vU9prwJyGfpOAXwMbddv+WmA19T+i/w6Mb3e9wJtLTSvL7xOqfHyBDwNPAysafqaM5PHt6fVI/XTj+8ryZuV43VeO384N236ubHc38M5WHs8B1Ht1+f+v63he3t9ro831fgW4vdR1HTC5Ydvjy3G/DziuCvWWx18AvtptuxE9vrmDRUREVF5OA0ZEROUlrCIiovISVhERUXkJq4iIqLyEVUREVF7CKiIiKi9hFRERlZewioiIyvv/Zk3TTxnnSIsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create linear regression\n",
    "regressor = ElasticNet(alpha=0.1, l1_ratio=0.1)\n",
    "\n",
    "# Fit/train ElasticNet\n",
    "regressor.fit(x_train,y_train)\n",
    "# Predict\n",
    "pred = regressor.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "names = list(df.columns.values)\n",
    "names.remove(\"mpg\")\n",
    "\n",
    "report_coef(\n",
    "  names,\n",
    "  regressor.coef_,\n",
    "  regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  L1/L2 in TensorFlow\n",
    "\n",
    "L1 and L2 regularization work by adding a weight penalty to the neural network training.  \n",
    "\n",
    "### This penalty push the connection weights to small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# TensorFlow with L1/L2 for Regression\n",
    "########################################\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import figure, show\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "df.drop('name',1,inplace=True)\n",
    "missing_median(df, 'horsepower')\n",
    "encode_text_dummy(df, 'origin')\n",
    "x,y = to_xy(df,\"mpg\")\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=45)\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can set three regularization params for Dense, Conv1D, Conv2D and Conv3D. \n",
    "\n",
    "keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "\n",
    "\n",
    "*** kernel_regularizer ***: Regularizer function applied to the kernel weights matrix. kernel regularizer will constantly decay the weights.\n",
    "\n",
    "*** activity_regularizer ***: Regularizer function applied to the output of the layer.  Activity regularizer will tend to make the output of the layer smaller.\n",
    "\n",
    "*** bias_regularizer ***: Regularizer function applied to the bias vector.\n",
    "\n",
    "https://keras.io/regularizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(50, input_dim=x.shape[1], activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "\n",
    "model.add(Dense(10, \n",
    "                kernel_regularizer=regularizers.l1(0.01),\n",
    "                activity_regularizer=regularizers.l2(0.01), activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 150565.0635 - val_loss: 39740.7288\n",
      "Epoch 2/100\n",
      " - 0s - loss: 17998.4396 - val_loss: 2926.6801\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1743.5122 - val_loss: 591.7831\n",
      "Epoch 4/100\n",
      " - 0s - loss: 625.8757 - val_loss: 583.3530\n",
      "Epoch 5/100\n",
      " - 0s - loss: 624.7468 - val_loss: 583.1245\n",
      "Epoch 6/100\n",
      " - 0s - loss: 624.5035 - val_loss: 582.8747\n",
      "Epoch 7/100\n",
      " - 0s - loss: 624.2406 - val_loss: 582.6081\n",
      "Epoch 8/100\n",
      " - 0s - loss: 623.9634 - val_loss: 582.3274\n",
      "Epoch 9/100\n",
      " - 0s - loss: 623.6702 - val_loss: 582.0360\n",
      "Epoch 10/100\n",
      " - 0s - loss: 623.3690 - val_loss: 581.7345\n",
      "Epoch 11/100\n",
      " - 0s - loss: 623.0545 - val_loss: 581.4260\n",
      "Epoch 12/100\n",
      " - 0s - loss: 622.7373 - val_loss: 581.1076\n",
      "Epoch 13/100\n",
      " - 0s - loss: 622.4089 - val_loss: 580.7861\n",
      "Epoch 14/100\n",
      " - 0s - loss: 622.0745 - val_loss: 580.4554\n",
      "Epoch 15/100\n",
      " - 0s - loss: 621.7345 - val_loss: 580.1171\n",
      "Epoch 16/100\n",
      " - 0s - loss: 621.3885 - val_loss: 579.7759\n",
      "Epoch 17/100\n",
      " - 0s - loss: 621.0351 - val_loss: 579.4333\n",
      "Epoch 18/100\n",
      " - 0s - loss: 620.6841 - val_loss: 579.0802\n",
      "Epoch 19/100\n",
      " - 0s - loss: 620.3238 - val_loss: 578.5367\n",
      "Epoch 20/100\n",
      " - 0s - loss: 510.5891 - val_loss: 415.9992\n",
      "Epoch 21/100\n",
      " - 0s - loss: 462.4559 - val_loss: 400.6582\n",
      "Epoch 22/100\n",
      " - 0s - loss: 451.6855 - val_loss: 384.5901\n",
      "Epoch 23/100\n",
      " - 0s - loss: 442.5350 - val_loss: 374.6150\n",
      "Epoch 24/100\n",
      " - 0s - loss: 440.4259 - val_loss: 391.1693\n",
      "Epoch 25/100\n",
      " - 0s - loss: 414.7237 - val_loss: 360.8217\n",
      "Epoch 26/100\n",
      " - 0s - loss: 395.6736 - val_loss: 348.0389\n",
      "Epoch 27/100\n",
      " - 0s - loss: 390.7757 - val_loss: 338.7950\n",
      "Epoch 28/100\n",
      " - 0s - loss: 381.9052 - val_loss: 330.5416\n",
      "Epoch 29/100\n",
      " - 0s - loss: 372.3731 - val_loss: 322.2422\n",
      "Epoch 30/100\n",
      " - 0s - loss: 366.9224 - val_loss: 317.6778\n",
      "Epoch 31/100\n",
      " - 0s - loss: 358.5977 - val_loss: 316.3136\n",
      "Epoch 32/100\n",
      " - 0s - loss: 348.9060 - val_loss: 302.8355\n",
      "Epoch 33/100\n",
      " - 0s - loss: 342.6954 - val_loss: 296.8911\n",
      "Epoch 34/100\n",
      " - 0s - loss: 341.8851 - val_loss: 296.5437\n",
      "Epoch 35/100\n",
      " - 0s - loss: 334.6502 - val_loss: 291.7496\n",
      "Epoch 36/100\n",
      " - 0s - loss: 323.2504 - val_loss: 281.0327\n",
      "Epoch 37/100\n",
      " - 0s - loss: 317.6242 - val_loss: 289.7237\n",
      "Epoch 38/100\n",
      " - 0s - loss: 323.8441 - val_loss: 287.5501\n",
      "Epoch 39/100\n",
      " - 0s - loss: 329.0099 - val_loss: 274.9715\n",
      "Epoch 40/100\n",
      " - 0s - loss: 309.6412 - val_loss: 261.9551\n",
      "Epoch 41/100\n",
      " - 0s - loss: 297.9664 - val_loss: 257.9289\n",
      "Epoch 42/100\n",
      " - 0s - loss: 294.8353 - val_loss: 255.3513\n",
      "Epoch 43/100\n",
      " - 0s - loss: 289.8775 - val_loss: 255.3773\n",
      "Epoch 44/100\n",
      " - 0s - loss: 288.9808 - val_loss: 275.2264\n",
      "Epoch 45/100\n",
      " - 0s - loss: 294.0443 - val_loss: 249.9663\n",
      "Epoch 46/100\n",
      " - 0s - loss: 293.7299 - val_loss: 240.2687\n",
      "Epoch 47/100\n",
      " - 0s - loss: 297.3840 - val_loss: 236.6382\n",
      "Epoch 48/100\n",
      " - 0s - loss: 286.6166 - val_loss: 238.8791\n",
      "Epoch 49/100\n",
      " - 0s - loss: 280.4015 - val_loss: 245.9404\n",
      "Epoch 50/100\n",
      " - 0s - loss: 265.3594 - val_loss: 225.0181\n",
      "Epoch 51/100\n",
      " - 0s - loss: 257.1702 - val_loss: 225.1807\n",
      "Epoch 52/100\n",
      " - 0s - loss: 253.0014 - val_loss: 233.4315\n",
      "Epoch 53/100\n",
      " - 0s - loss: 252.6643 - val_loss: 215.2013\n",
      "Epoch 54/100\n",
      " - 0s - loss: 247.2475 - val_loss: 212.3451\n",
      "Epoch 55/100\n",
      " - 0s - loss: 241.4116 - val_loss: 209.0374\n",
      "Epoch 56/100\n",
      " - 0s - loss: 236.7735 - val_loss: 206.6439\n",
      "Epoch 57/100\n",
      " - 0s - loss: 234.8050 - val_loss: 206.7698\n",
      "Epoch 58/100\n",
      " - 0s - loss: 236.9018 - val_loss: 206.8916\n",
      "Epoch 59/100\n",
      " - 0s - loss: 230.7003 - val_loss: 206.1617\n",
      "Epoch 60/100\n",
      " - 0s - loss: 227.8237 - val_loss: 199.8863\n",
      "Epoch 61/100\n",
      " - 0s - loss: 229.0702 - val_loss: 200.7513\n",
      "Epoch 62/100\n",
      " - 0s - loss: 226.7328 - val_loss: 200.7202\n",
      "Epoch 63/100\n",
      " - 0s - loss: 223.5785 - val_loss: 190.1478\n",
      "Epoch 64/100\n",
      " - 0s - loss: 215.2994 - val_loss: 186.3259\n",
      "Epoch 65/100\n",
      " - 0s - loss: 213.1337 - val_loss: 193.6402\n",
      "Epoch 66/100\n",
      " - 0s - loss: 216.9370 - val_loss: 190.0695\n",
      "Epoch 67/100\n",
      " - 0s - loss: 222.9273 - val_loss: 184.4563\n",
      "Epoch 68/100\n",
      " - 0s - loss: 217.4891 - val_loss: 177.0880\n",
      "Epoch 69/100\n",
      " - 0s - loss: 217.7278 - val_loss: 180.4624\n",
      "Epoch 70/100\n",
      " - 0s - loss: 211.3867 - val_loss: 173.6890\n",
      "Epoch 71/100\n",
      " - 0s - loss: 200.6092 - val_loss: 180.4275\n",
      "Epoch 72/100\n",
      " - 0s - loss: 203.7094 - val_loss: 179.5959\n",
      "Epoch 73/100\n",
      " - 0s - loss: 208.4546 - val_loss: 182.2246\n",
      "Epoch 74/100\n",
      " - 0s - loss: 205.7232 - val_loss: 172.5331\n",
      "Epoch 75/100\n",
      " - 0s - loss: 204.7037 - val_loss: 170.0848\n",
      "Epoch 76/100\n",
      " - 0s - loss: 195.4237 - val_loss: 171.1304\n",
      "Epoch 77/100\n",
      " - 0s - loss: 186.2144 - val_loss: 180.0505\n",
      "Epoch 78/100\n",
      " - 0s - loss: 190.0461 - val_loss: 169.6399\n",
      "Epoch 79/100\n",
      " - 0s - loss: 186.1119 - val_loss: 170.7303\n",
      "Epoch 80/100\n",
      " - 0s - loss: 180.9315 - val_loss: 161.9153\n",
      "Epoch 81/100\n",
      " - 0s - loss: 180.8706 - val_loss: 178.3211\n",
      "Epoch 82/100\n",
      " - 0s - loss: 192.3372 - val_loss: 189.1177\n",
      "Epoch 83/100\n",
      " - 0s - loss: 186.5223 - val_loss: 159.1443\n",
      "Epoch 84/100\n",
      " - 0s - loss: 178.1228 - val_loss: 155.9973\n",
      "Epoch 85/100\n",
      " - 0s - loss: 180.0518 - val_loss: 158.7517\n",
      "Epoch 86/100\n",
      " - 0s - loss: 179.1590 - val_loss: 156.4059\n",
      "Epoch 87/100\n",
      " - 0s - loss: 184.3855 - val_loss: 177.8808\n",
      "Epoch 88/100\n",
      " - 0s - loss: 188.4211 - val_loss: 164.9273\n",
      "Epoch 89/100\n",
      " - 0s - loss: 183.3759 - val_loss: 177.0554\n",
      "Epoch 00089: early stopping\n",
      "Final score (RMSE): 7.095132350921631\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(1))   #  output layer\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=100)\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Layer  (A dedicated layer for regularization)\n",
    "\n",
    "#### Each dropout layer will drop neurons in its previous layer.\n",
    "\n",
    "*** To create a dropout layer, specify dropout probability.   ***  The dropout probability indicates the likelihood of a neuron dropping out for every batch during training. Typically this value is 0.1 to 0.5. \n",
    "\n",
    "*** Actually, a certain percentage of neurons will be masked during each training iteration.  All neurons return after training is complete.*** \n",
    "\n",
    "\n",
    "Animation that shows how [dropout works](https://yusugomori.com/projects/deep-learning/dropout-relu)\n",
    "\n",
    "#### A dropout layer can be added between any two hidden layers to reduce overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Code with Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin-1</th>\n",
       "      <th>origin-2</th>\n",
       "      <th>origin-3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>429.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>4341</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>454.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>4354</td>\n",
       "      <td>9.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>440.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4312</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>455.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>4425</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>390.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3850</td>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>383.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>3563</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>340.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>3609</td>\n",
       "      <td>8.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>400.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3761</td>\n",
       "      <td>9.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>455.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>3086</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24.0</td>\n",
       "      <td>4</td>\n",
       "      <td>113.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2372</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22.0</td>\n",
       "      <td>6</td>\n",
       "      <td>198.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2833</td>\n",
       "      <td>15.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6</td>\n",
       "      <td>199.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2774</td>\n",
       "      <td>15.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>200.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2587</td>\n",
       "      <td>16.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2130</td>\n",
       "      <td>14.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1835</td>\n",
       "      <td>20.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0   18.0          8         307.0       130.0    3504          12.0    70   \n",
       "1   15.0          8         350.0       165.0    3693          11.5    70   \n",
       "2   18.0          8         318.0       150.0    3436          11.0    70   \n",
       "3   16.0          8         304.0       150.0    3433          12.0    70   \n",
       "4   17.0          8         302.0       140.0    3449          10.5    70   \n",
       "5   15.0          8         429.0       198.0    4341          10.0    70   \n",
       "6   14.0          8         454.0       220.0    4354           9.0    70   \n",
       "7   14.0          8         440.0       215.0    4312           8.5    70   \n",
       "8   14.0          8         455.0       225.0    4425          10.0    70   \n",
       "9   15.0          8         390.0       190.0    3850           8.5    70   \n",
       "10  15.0          8         383.0       170.0    3563          10.0    70   \n",
       "11  14.0          8         340.0       160.0    3609           8.0    70   \n",
       "12  15.0          8         400.0       150.0    3761           9.5    70   \n",
       "13  14.0          8         455.0       225.0    3086          10.0    70   \n",
       "14  24.0          4         113.0        95.0    2372          15.0    70   \n",
       "15  22.0          6         198.0        95.0    2833          15.5    70   \n",
       "16  18.0          6         199.0        97.0    2774          15.5    70   \n",
       "17  21.0          6         200.0        85.0    2587          16.0    70   \n",
       "18  27.0          4          97.0        88.0    2130          14.5    70   \n",
       "19  26.0          4          97.0        46.0    1835          20.5    70   \n",
       "\n",
       "    origin-1  origin-2  origin-3  \n",
       "0          1         0         0  \n",
       "1          1         0         0  \n",
       "2          1         0         0  \n",
       "3          1         0         0  \n",
       "4          1         0         0  \n",
       "5          1         0         0  \n",
       "6          1         0         0  \n",
       "7          1         0         0  \n",
       "8          1         0         0  \n",
       "9          1         0         0  \n",
       "10         1         0         0  \n",
       "11         1         0         0  \n",
       "12         1         0         0  \n",
       "13         1         0         0  \n",
       "14         0         0         1  \n",
       "15         1         0         0  \n",
       "16         1         0         0  \n",
       "17         1         0         0  \n",
       "18         0         0         1  \n",
       "19         0         1         0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################\n",
    "# TensorFlow with Dropout for Regression\n",
    "############################################\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import figure, show\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "\n",
    "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "df.drop('name',1,inplace=True)\n",
    "\n",
    "missing_median(df, 'horsepower')\n",
    "\n",
    "encode_text_dummy(df, 'origin')\n",
    "\n",
    "df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = to_xy(df,\"mpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298 samples, validate on 100 samples\n",
      "Epoch 1/1000\n",
      " - 0s - loss: 70310.7269 - val_loss: 14136.8774\n",
      "Epoch 2/1000\n",
      " - 0s - loss: 36647.8025 - val_loss: 19766.2130\n",
      "Epoch 3/1000\n",
      " - 0s - loss: 30283.2970 - val_loss: 533.8392\n",
      "Epoch 4/1000\n",
      " - 0s - loss: 27454.0324 - val_loss: 718.6131\n",
      "Epoch 5/1000\n",
      " - 0s - loss: 22808.3245 - val_loss: 5727.1088\n",
      "Epoch 6/1000\n",
      " - 0s - loss: 17775.4137 - val_loss: 1332.6250\n",
      "Epoch 7/1000\n",
      " - 0s - loss: 12805.6974 - val_loss: 772.2804\n",
      "Epoch 8/1000\n",
      " - 0s - loss: 10127.5676 - val_loss: 386.0115\n",
      "Epoch 9/1000\n",
      " - 0s - loss: 6938.6995 - val_loss: 454.2722\n",
      "Epoch 10/1000\n",
      " - 0s - loss: 6147.2436 - val_loss: 144.7602\n",
      "Epoch 11/1000\n",
      " - 0s - loss: 5046.8344 - val_loss: 221.2821\n",
      "Epoch 12/1000\n",
      " - 0s - loss: 4455.0360 - val_loss: 506.8088\n",
      "Epoch 13/1000\n",
      " - 0s - loss: 4092.9713 - val_loss: 597.9951\n",
      "Epoch 14/1000\n",
      " - 0s - loss: 3666.4743 - val_loss: 398.2632\n",
      "Epoch 15/1000\n",
      " - 0s - loss: 3249.6554 - val_loss: 264.7143\n",
      "Epoch 00015: early stopping\n",
      "Final score (RMSE): 16.270044326782227\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=45)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=x.shape[1]))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=1000)\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 50)                500       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,046\n",
      "Trainable params: 2,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection:  How 500 and 1275 were calcuated?\n",
    "\n",
    "1275 = 50*25 + 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "* [Google Colab](https://colab.research.google.com/) - Free web based platform that includes Python, Juypter Notebooks, and TensorFlow with free GPU support.  No setup needed.\n",
    "* [IBM Cognitive Class Labs](https://www.datascientistworkbench.com) - Free web based platform that includes Python, Juypter Notebooks, and TensorFlow.  No setup needed.\n",
    "* [Python Anaconda](https://www.continuum.io/downloads) - Python distribution that includes many data science packages, such as Numpy, Scipy, Scikit-Learn, Pandas, and much more.\n",
    "* [TensorFlow](https://www.tensorflow.org/) - Google's mathematics package for deep learning.\n",
    "* [Kaggle](https://www.kaggle.com/) - Competitive data science.  Good source of sample data.\n",
    "* T81-558: Applications of Deep Neural Networks. Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
